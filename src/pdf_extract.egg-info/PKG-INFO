Metadata-Version: 2.4
Name: pdf-extract
Version: 0.1.0
Summary: PDF â†’ NuExtract (OpenAI-compatible) JSON extraction pipeline
Author: you
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.31
Requires-Dist: pypdf>=4.2.0
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: tomli>=2.0.1; python_version < "3.11"

# PDF â†’ LLM Extract

Simple pipeline that reads a research paper (PDF) and extracts structured data (JSON) using a local **OpenAI-compatible LLM server** such as `llama.cpp` running a NuMind **NuExtract** model.

---

## ðŸ§  How It Works

1. The PDF is read with `pypdf` (limited by `pdf.max_pages`).
2. A NuExtract-style prompt is built:

   * `# Template:` â†’ your JSON schema
   * `# Instructions:` â†’ short English rules
   * `# Context:` â†’ the PDF text
3. The model is called once (or over multiple chunks if needed).
4. All partial JSON results are merged automatically.

The model returns **only JSON**, e.g.:

```json
{"n_included": 15247, "countries": ["United Kingdom"]}
```

---

## âš™ï¸ Configuration

Everything lives in `config.toml`:

### `[llm]`

* `base_url` â†’ your local server (e.g. `http://127.0.0.1:8080/v1`)
* `model` â†’ model name (e.g. `numind/NuExtract-2.0-4B-GGUF`)
* `temperature` (default 0.0)
* `max_tokens` (default 256)
* `use_grammar` (usually `false`)

### `[pdf]`

* `path` â†’ path to your PDF
* `max_pages` â†’ how many pages to read

### `[task]`

* `template_json` â†’ JSON schema of fields to extract
* `instructions` â†’ clear extraction rules

Example:

```toml
[task]
template_json = '{"n_included":"integer","countries":["verbatim-string"]}'
instructions  = '''
Extract the number of INCLUDED participants and the countries they came from.
Include only final included participants. Exclude screened, excluded, or author locations.
Return each country verbatim. Deduplicate.
'''
```

---

## ðŸš€ Running

Start the LLM server:

```
llama-server -hf numind/NuExtract-2.0-4B-GGUF -fa on -ngl 999 --port 8080 --ctx-size 20000
```

> `--ctx-size` defines max context length. Increase for longer papers (slower, more VRAM/CPU).

Then run the extractor:

```
pip install -e .
python -m src.main
```

Output appears as JSON in the console.

---

## ðŸ§© Adding New Fields

No code changes required:

1. Add new fields to `template_json`.
2. Describe them in `instructions`.
3. Re-run `python -m src.main`.

Example new field:

```toml
template_json = '{"n_included":"integer","countries":["string"],"age_mean":"float"}'
```

---

## âš¡ Tips

* **Speed up:** lower `pdf.max_pages`, use 4B quantized model, smaller `--ctx-size`.
* **Improve accuracy:** raise `max_pages`, `max_tokens`, or context size.
* **Evidence extraction:** add an extra field like `"evidence": ["string"]` and update the instructions accordingly.
